{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\R6RW5M6\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: 'sckit-learn,transformers,seaborn,pandas': Expected end or semicolon (after name and no valid version specifier)\n",
      "    sckit-learn,transformers,seaborn,pandas\n",
      "               ^\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61adf400",
   "metadata": {},
   "source": [
    "# Marathi Sentiment Analysis: Hybrid XLM-RoBERTa + CNN Architecture\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a comprehensive sentiment analysis system for Marathi social media text using:\n",
    "- **Traditional ML Baselines**: SVM, Random Forest, Logistic Regression, KNN\n",
    "- **Deep Learning Baselines**: LSTM, BiLSTM, CNN, Multi-CNN\n",
    "- **PLM Fine-tuning**: XLM-RoBERTa\n",
    "- **Hybrid Architecture**: XLM-RoBERTa + CNN with Mean Pooling\n",
    "\n",
    "### Dataset: MahaSent\n",
    "- **Total Samples**: 60,864 (perfectly balanced 3-class)\n",
    "- **Train**: 48,114 | **Test**: 6,750 | **Val**: 6,000\n",
    "- **Labels**: Negative (-1), Neutral (0), Positive (1)\n",
    "- **Language**: Marathi (Devanagari script)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452ae7e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "Install required packages and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91373d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our pre-built modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.abspath('src'))\n",
    "\n",
    "# Import project modules\n",
    "from src.config import Config\n",
    "from src.data_loader import get_dataloaders_from_config\n",
    "from src.models.hybrid_model import HybridSentimentModel\n",
    "from src.train import train_epoch, validate, setup_training\n",
    "from src.evaluate import evaluate_model, calculate_all_metrics\n",
    "from src.visualize import plot_training_history, plot_confusion_matrix, plot_per_class_metrics\n",
    "from src.utils.logger import ExperimentLogger\n",
    "\n",
    "# Standard imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "print(\"‚úÖ All modules imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc817bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for Marathi Sentiment Analysis project.\"\"\"\n",
    "    \n",
    "    # Dataset paths\n",
    "    data_dir: str = \".\"\n",
    "    train_file: str = \"MahaSent_All_Train.csv\"\n",
    "    test_file: str = \"MahaSent_All_Test.csv\"\n",
    "    val_file: str = \"MahaSent_All_Val.csv\"\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    plm_name: str = \"xlm-roberta-base\"\n",
    "    max_seq_length: int = 256\n",
    "    batch_size: int = 64\n",
    "    learning_rate_hybrid: float = 1e-4\n",
    "    learning_rate_plm: float = 2e-5\n",
    "    num_epochs: int = 20\n",
    "    early_stopping_patience: int = 3\n",
    "    \n",
    "    # Architecture parameters\n",
    "    plm_hidden_size: int = 768vc\n",
    "    cnn_out_channels: int = 256\n",
    "    cnn_kernel_size: int = 3\n",
    "    dense_hidden_size: int = 512\n",
    "    dropout_rate: float = 0.3\n",
    "    num_classes: int = 3\n",
    "    \n",
    "    # Training parameters\n",
    "    warmup_steps: int = 500\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Traditional ML parameters\n",
    "    tfidf_max_features: int = 5000\n",
    "    \n",
    "    # DL baseline parameters\n",
    "    embedding_dim: int = 300\n",
    "    lstm_hidden_size: int = 128\n",
    "    vocab_size: int = 10000\n",
    "    \n",
    "    # Paths\n",
    "    results_dir: str = \"results\"\n",
    "    figures_dir: str = \"results/figures\"\n",
    "    models_dir: str = \"results/models\"\n",
    "    \n",
    "    # Label mapping\n",
    "    label_map: Dict[int, int] = field(default_factory=lambda: {-1: 0, 0: 1, 1: 2})\n",
    "    label_names: List[str] = field(default_factory=lambda: ['Negative', 'Neutral', 'Positive'])\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = field(default_factory=lambda: torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert config to dictionary.\"\"\"\n",
    "        return {\n",
    "            'plm_name': self.plm_name,\n",
    "            'max_seq_length': self.max_seq_length,\n",
    "            'batch_size': self.batch_size,\n",
    "            'learning_rate_hybrid': self.learning_rate_hybrid,\n",
    "            'learning_rate_plm': self.learning_rate_plm,\n",
    "            'num_epochs': self.num_epochs,\n",
    "            'early_stopping_patience': self.early_stopping_patience,\n",
    "        }\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        Path(self.results_dir).mkdir(exist_ok=True)\n",
    "        Path(self.figures_dir).mkdir(exist_ok=True)\n",
    "        Path(self.models_dir).mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration initialized\")\n",
    "print(f\"   Device: {config.device}\")\n",
    "print(f\"   PLM: {config.plm_name}\")\n",
    "print(f\"   Batch size: {config.batch_size}\")\n",
    "print(f\"   Max sequence length: {config.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486e168",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "Load the MahaSent dataset and verify its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe532fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(config.data_dir, config.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(config.data_dir, config.test_file))\n",
    "    val_df = pd.read_csv(os.path.join(config.data_dir, config.val_file))\n",
    "    \n",
    "    print(\"‚úÖ Datasets loaded successfully!\")\n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"   Train: {len(train_df):,} samples\")\n",
    "    print(f\"   Test:  {len(test_df):,} samples\")\n",
    "    print(f\"   Val:   {len(val_df):,} samples\")\n",
    "    print(f\"   Total: {len(train_df) + len(test_df) + len(val_df):,} samples\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error loading datasets: {e}\")\n",
    "    print(f\"   Please ensure CSV files are in: {config.data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class balance\n",
    "print(\"\\nüìà Label Distribution:\")\n",
    "print(\"\\nTrain Set:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print(f\"Balance: {train_df['label'].value_counts(normalize=True).values}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nVal Set:\")\n",
    "print(val_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüìù Sample Texts by Sentiment:\")\n",
    "for label in [-1, 0, 1]:\n",
    "    sentiment_name = config.label_names[config.label_map[label]]\n",
    "    print(f\"\\n{sentiment_name} (Label {label}):\")\n",
    "    samples = train_df[train_df['label'] == label]['text'].head(2).values\n",
    "    for i, text in enumerate(samples, 1):\n",
    "        print(f\"  {i}. {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1230fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "val_df['text_length'] = val_df['text'].str.len()\n",
    "\n",
    "print(\"\\nüìè Text Length Statistics (characters):\")\n",
    "for name, df in [('Train', train_df), ('Test', test_df), ('Val', val_df)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {df['text_length'].mean():.1f}\")\n",
    "    print(f\"  Median: {df['text_length'].median():.1f}\")\n",
    "    print(f\"  Min: {df['text_length'].min()}\")\n",
    "    print(f\"  Max: {df['text_length'].max()}\")\n",
    "    print(f\"  Std: {df['text_length'].std():.1f}\")\n",
    "\n",
    "# Visualize text length distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for idx, (name, df) in enumerate([('Train', train_df), ('Test', test_df), ('Val', val_df)]):\n",
    "    axes[idx].hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{name} Set - Text Length Distribution')\n",
    "    axes[idx].set_xlabel('Character Count')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].axvline(df['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"text_length\"].mean():.1f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'text_length_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Exploratory analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a4856",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing and Tokenization\n",
    "\n",
    "Implement Marathi-specific preprocessing and create PyTorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe3fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_marathi_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess Marathi text for sentiment analysis.\n",
    "    \n",
    "    Args:\n",
    "        text: Input Marathi text\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http(s)?://\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Keep text after hashtags (remove # but keep word)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"üîÑ Preprocessing text data...\")\n",
    "train_df['text_clean'] = train_df['text'].apply(preprocess_marathi_text)\n",
    "test_df['text_clean'] = test_df['text'].apply(preprocess_marathi_text)\n",
    "val_df['text_clean'] = val_df['text'].apply(preprocess_marathi_text)\n",
    "\n",
    "# Map labels: -1‚Üí0, 0‚Üí1, 1‚Üí2\n",
    "train_df['label_mapped'] = train_df['label'].map(config.label_map)\n",
    "test_df['label_mapped'] = test_df['label'].map(config.label_map)\n",
    "val_df['label_mapped'] = val_df['label'].map(config.label_map)\n",
    "\n",
    "print(\"‚úÖ Text preprocessing complete!\")\n",
    "print(f\"\\nSample preprocessed text:\")\n",
    "print(f\"Original: {train_df['text'].iloc[0][:80]}...\")\n",
    "print(f\"Cleaned:  {train_df['text_clean'].iloc[0][:80]}...\")\n",
    "print(f\"Label mapping: {train_df['label'].iloc[0]} ‚Üí {train_df['label_mapped'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XLM-RoBERTa tokenizer\n",
    "print(\"üîÑ Loading XLM-RoBERTa tokenizer...\")\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(config.plm_name)\n",
    "print(f\"‚úÖ Tokenizer loaded: {config.plm_name}\")\n",
    "\n",
    "# PyTorch Dataset class\n",
    "class MarathiSentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Marathi sentiment analysis.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of preprocessed text strings\n",
    "        labels: List of mapped labels (0, 1, 2)\n",
    "        tokenizer: XLMRobertaTokenizer instance\n",
    "        max_length: Maximum sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        labels: List[int], \n",
    "        tokenizer: XLMRobertaTokenizer, \n",
    "        max_length: int = 256\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MarathiSentimentDataset(\n",
    "    train_df['text_clean'].tolist(),\n",
    "    train_df['label_mapped'].tolist(),\n",
    "    tokenizer,\n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "val_dataset = MarathiSentimentDataset(\n",
    "    val_df['text_clean'].tolist(),\n",
    "    val_df['label_mapped'].tolist(),\n",
    "    tokenizer,\n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "test_dataset = MarathiSentimentDataset(\n",
    "    test_df['text_clean'].tolist(),\n",
    "    test_df['label_mapped'].tolist(),\n",
    "    tokenizer,\n",
    "    config.max_seq_length\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created:\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} samples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f923314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DataLoaders created:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches:   {len(val_loader)}\")\n",
    "print(f\"   Test batches:  {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nüîç Sample batch shapes:\")\n",
    "print(f\"   Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"   Attention Mask: {sample_batch['attention_mask'].shape}\")\n",
    "print(f\"   Labels: {sample_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5923246",
   "metadata": {},
   "source": [
    "## 4. Traditional ML Baselines\n",
    "\n",
    "Train and evaluate SVM, Random Forest, Logistic Regression, and KNN with TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1764b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features\n",
    "print(\"üîÑ Extracting TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=config.tfidf_max_features)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['text_clean'])\n",
    "X_val_tfidf = vectorizer.transform(val_df['text_clean'])\n",
    "X_test_tfidf = vectorizer.transform(test_df['text_clean'])\n",
    "\n",
    "y_train = train_df['label_mapped'].values\n",
    "y_val = val_df['label_mapped'].values\n",
    "y_test = test_df['label_mapped'].values\n",
    "\n",
    "print(f\"‚úÖ TF-IDF features extracted:\")\n",
    "print(f\"   Feature dimensions: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"   Train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"   Val shape: {X_val_tfidf.shape}\")\n",
    "print(f\"   Test shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "traditional_ml_results = {}\n",
    "\n",
    "# Define models\n",
    "traditional_models = {\n",
    "    'SVM': SVC(kernel='linear', C=1.0, class_weight='balanced', random_state=SEED),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, min_samples_split=6, random_state=SEED),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training Traditional ML Models...\\n\")\n",
    "\n",
    "for model_name, model in traditional_models.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_val_pred, average='macro')\n",
    "    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    traditional_ml_results[model_name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision,\n",
    "        'recall_macro': recall,\n",
    "        'f1_macro': f1,\n",
    "        'precision_weighted': precision_w,\n",
    "        'recall_weighted': recall_w,\n",
    "        'f1_weighted': f1_w,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {f1:.4f}\")\n",
    "    print(f\"  F1-Score (Weighted): {f1_w:.4f}\")\n",
    "    print(f\"  Training time: {train_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Traditional ML training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf7ad7",
   "metadata": {},
   "source": [
    "## 5. Hybrid Model Architecture\n",
    "\n",
    "Implement the XLM-RoBERTa + CNN hybrid model with mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSentimentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid XLM-RoBERTa + CNN model for Marathi sentiment analysis.\n",
    "    \n",
    "    Architecture:\n",
    "        1. XLM-RoBERTa base for contextual embeddings (768-dim)\n",
    "        2. Mean pooling over sequence (attention-masked)\n",
    "        3. Conv1D for local pattern extraction (768‚Üí256)\n",
    "        4. Concatenate pooled + CNN features (1024-dim)\n",
    "        5. Dense classification layers (1024‚Üí512‚Üí3)\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object with hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super(HybridSentimentModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained XLM-RoBERTa\n",
    "        self.plm = XLMRobertaModel.from_pretrained(config.plm_name)\n",
    "        \n",
    "        # CNN for local patterns\n",
    "        self.cnn = nn.Conv1d(\n",
    "            in_channels=config.plm_hidden_size,\n",
    "            out_channels=config.cnn_out_channels,\n",
    "            kernel_size=config.cnn_kernel_size,\n",
    "            padding='same'\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.fc1 = nn.Linear(config.plm_hidden_size + config.cnn_out_channels, config.dense_hidden_size)\n",
    "        self.fc2 = nn.Linear(config.dense_hidden_size, config.num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def mean_pooling(self, token_embeddings: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply mean pooling with attention mask.\n",
    "        \n",
    "        Args:\n",
    "            token_embeddings: Token embeddings (batch_size, seq_len, hidden_size)\n",
    "            attention_mask: Attention mask (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            Pooled embeddings (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            Logits (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Get PLM embeddings\n",
    "        plm_output = self.plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = plm_output.last_hidden_state  # (batch, seq_len, 768)\n",
    "        \n",
    "        # Mean pooling\n",
    "        pooled_output = self.mean_pooling(token_embeddings, attention_mask)  # (batch, 768)\n",
    "        \n",
    "        # CNN for local patterns\n",
    "        # Transpose for Conv1D: (batch, seq_len, 768) ‚Üí (batch, 768, seq_len)\n",
    "        cnn_input = token_embeddings.permute(0, 2, 1)\n",
    "        cnn_output = self.cnn(cnn_input)  # (batch, 256, seq_len)\n",
    "        cnn_output = F.relu(cnn_output)\n",
    "        cnn_pooled = F.max_pool1d(cnn_output, kernel_size=cnn_output.size(2)).squeeze(2)  # (batch, 256)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([pooled_output, cnn_pooled], dim=1)  # (batch, 1024)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = self.dropout(combined)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)  # (batch, 3)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "hybrid_model = HybridSentimentModel(config).to(device)\n",
    "print(f\"‚úÖ Hybrid model initialized\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in hybrid_model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in hybrid_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbb6c1",
   "metadata": {},
   "source": [
    "## 6. Training Functions\n",
    "\n",
    "Implement training and validation loops with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_samples:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        data_loader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: Device to validate on\n",
    "        \n",
    "    Returns:\n",
    "        Average loss, accuracy, predictions, and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457da72",
   "metadata": {},
   "source": [
    "## 7. Train Hybrid Model\n",
    "\n",
    "Train the hybrid XLM-RoBERTa + CNN model with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(hybrid_model.parameters(), lr=config.learning_rate_hybrid, weight_decay=config.weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = os.path.join(config.models_dir, 'hybrid_model_best.pt')\n",
    "\n",
    "print(\"üöÄ Starting Hybrid Model Training...\\n\")\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(hybrid_model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(hybrid_model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': hybrid_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, best_model_path)\n",
    "        print(f\"  ‚úÖ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ‚è≥ No improvement ({patience_counter}/{config.early_stopping_patience})\")\n",
    "        \n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"\\nüõë Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Hybrid model training complete!\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65900ec",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Evaluate the hybrid model on the test set and calculate comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "hybrid_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = validate(hybrid_model, test_loader, criterion, device)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average='macro'\n",
    ")\n",
    "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average='weighted'\n",
    ")\n",
    "\n",
    "# Per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ HYBRID MODEL TEST SET RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy:            {test_acc:.4f}\")\n",
    "print(f\"  Precision (Macro):   {precision_macro:.4f}\")\n",
    "print(f\"  Recall (Macro):      {recall_macro:.4f}\")\n",
    "print(f\"  F1-Score (Macro):    {f1_macro:.4f}\")\n",
    "print(f\"  F1-Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-Class Metrics:\")\n",
    "for i, class_name in enumerate(config.label_names):\n",
    "    print(f\"\\n  {class_name}:\")\n",
    "    print(f\"    Precision: {precision_per_class[i]:.4f}\")\n",
    "    print(f\"    Recall:    {recall_per_class[i]:.4f}\")\n",
    "    print(f\"    F1-Score:  {f1_per_class[i]:.4f}\")\n",
    "    print(f\"    Support:   {support[i]}\")\n",
    "\n",
    "# Store hybrid model results\n",
    "hybrid_results = {\n",
    "    'accuracy': test_acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'test_preds': test_preds,\n",
    "    'test_labels': test_labels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773d76f",
   "metadata": {},
   "source": [
    "## 9. Results Visualization\n",
    "\n",
    "Create comprehensive visualizations comparing all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training History\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs_range, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(epochs_range, history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Confusion Matrix for Hybrid Model\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=config.label_names,\n",
    "            yticklabels=config.label_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Hybrid XLM-RoBERTa + CNN', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Comparison Bar Chart\n",
    "model_names = list(traditional_ml_results.keys()) + ['Hybrid XLM-R + CNN']\n",
    "accuracies = [traditional_ml_results[name]['accuracy'] for name in traditional_ml_results.keys()] + [hybrid_results['accuracy']]\n",
    "f1_scores = [traditional_ml_results[name]['f1_macro'] for name in traditional_ml_results.keys()] + [hybrid_results['f1_macro']]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8, color='skyblue')\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score (Macro)', alpha=0.8, color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Comparison: Traditional ML vs Hybrid Deep Learning', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.0])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Model comparison chart saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a294564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Per-Class Metrics for Hybrid Model\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "}, index=config.label_names)\n",
    "\n",
    "ax = metrics_df.plot(kind='bar', figsize=(12, 6), width=0.8, alpha=0.8)\n",
    "ax.set_xlabel('Sentiment Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Performance: Hybrid XLM-RoBERTa + CNN', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticklabels(config.label_names, rotation=0)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.0])\n",
    "\n",
    "# Add value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'per_class_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Per-class metrics chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac4aa8",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Analyze misclassifications to gain insights for model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d486fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(test_preds != test_labels)[0]\n",
    "correct_indices = np.where(test_preds == test_labels)[0]\n",
    "\n",
    "print(f\"üìä Error Analysis Summary:\")\n",
    "print(f\"   Total test samples: {len(test_labels)}\")\n",
    "print(f\"   Correct predictions: {len(correct_indices)} ({len(correct_indices)/len(test_labels)*100:.2f}%)\")\n",
    "print(f\"   Misclassifications: {len(misclassified_indices)} ({len(misclassified_indices)/len(test_labels)*100:.2f}%)\")\n",
    "\n",
    "# Analyze misclassification patterns\n",
    "print(f\"\\nüîç Misclassification Patterns:\")\n",
    "for true_label in range(3):\n",
    "    for pred_label in range(3):\n",
    "        if true_label != pred_label:\n",
    "            count = np.sum((test_labels[misclassified_indices] == true_label) & \n",
    "                          (test_preds[misclassified_indices] == pred_label))\n",
    "            if count > 0:\n",
    "                print(f\"   {config.label_names[true_label]} ‚Üí {config.label_names[pred_label]}: {count} samples\")\n",
    "\n",
    "# Display sample misclassifications\n",
    "print(f\"\\n‚ùå Sample Misclassifications:\\n\")\n",
    "for i in misclassified_indices[:10]:\n",
    "    true_label = test_labels[i]\n",
    "    pred_label = test_preds[i]\n",
    "    text = test_df.iloc[i]['text_clean']\n",
    "    \n",
    "    print(f\"True: {config.label_names[true_label]} | Pred: {config.label_names[pred_label]}\")\n",
    "    print(f\"Text: {text[:120]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Text length analysis of misclassifications\n",
    "misclassified_lengths = test_df.iloc[misclassified_indices]['text_length'].values\n",
    "correct_lengths = test_df.iloc[correct_indices]['text_length'].values\n",
    "\n",
    "print(f\"\\nüìè Text Length Analysis:\")\n",
    "print(f\"   Misclassified - Mean: {misclassified_lengths.mean():.1f}, Median: {np.median(misclassified_lengths):.1f}\")\n",
    "print(f\"   Correct - Mean: {correct_lengths.mean():.1f}, Median: {np.median(correct_lengths):.1f}\")\n",
    "\n",
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(correct_lengths, bins=50, alpha=0.6, label='Correct', color='green', edgecolor='black')\n",
    "plt.hist(misclassified_lengths, bins=50, alpha=0.6, label='Misclassified', color='red', edgecolor='black')\n",
    "plt.xlabel('Text Length (characters)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Text Length Distribution: Correct vs Misclassified Predictions', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config.figures_dir, 'error_analysis_length.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Error analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbb28e",
   "metadata": {},
   "source": [
    "## 11. Final Summary and Export Results\n",
    "\n",
    "Compile all results and save to files for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30abc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "results_summary = {\n",
    "    'Traditional ML': {},\n",
    "    'Hybrid Model': {\n",
    "        'Accuracy': hybrid_results['accuracy'],\n",
    "        'Precision (Macro)': hybrid_results['precision_macro'],\n",
    "        'Recall (Macro)': hybrid_results['recall_macro'],\n",
    "        'F1-Score (Macro)': hybrid_results['f1_macro'],\n",
    "        'F1-Score (Weighted)': hybrid_results['f1_weighted']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add traditional ML results\n",
    "for model_name, results in traditional_ml_results.items():\n",
    "    results_summary['Traditional ML'][model_name] = {\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'F1-Score (Macro)': results['f1_macro'],\n",
    "        'F1-Score (Weighted)': results['f1_weighted'],\n",
    "        'Training Time (s)': results['train_time']\n",
    "    }\n",
    "\n",
    "# Print final summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üèÜ FINAL RESULTS SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Traditional ML Models (TF-IDF Features):\")\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'F1-Macro':<12} {'F1-Weighted':<12} {'Time (s)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for model_name, metrics in results_summary['Traditional ML'].items():\n",
    "    print(f\"{model_name:<20} {metrics['Accuracy']:<12.4f} {metrics['F1-Score (Macro)']:<12.4f} \"\n",
    "          f\"{metrics['F1-Score (Weighted)']:<12.4f} {metrics['Training Time (s)']:<10.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Hybrid Deep Learning Model (XLM-RoBERTa + CNN):\")\n",
    "print(f\"{'Metric':<30} {'Score':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for metric, value in results_summary['Hybrid Model'].items():\n",
    "    print(f\"{metric:<30} {value:<12.4f}\")\n",
    "\n",
    "# Find best model\n",
    "best_traditional = max(results_summary['Traditional ML'].items(), \n",
    "                      key=lambda x: x[1]['F1-Score (Macro)'])\n",
    "hybrid_f1 = results_summary['Hybrid Model']['F1-Score (Macro)']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ü•á BEST MODELS:\")\n",
    "print(f\"   Traditional ML: {best_traditional[0]} (F1-Macro: {best_traditional[1]['F1-Score (Macro)']:.4f})\")\n",
    "print(f\"   Deep Learning:  Hybrid XLM-R + CNN (F1-Macro: {hybrid_f1:.4f})\")\n",
    "print(f\"   Improvement:    {(hybrid_f1 - best_traditional[1]['F1-Score (Macro)'])*100:.2f}% increase\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(results_summary['Traditional ML'].keys()) + ['Hybrid XLM-R + CNN'],\n",
    "    'Accuracy': [v['Accuracy'] for v in results_summary['Traditional ML'].values()] + [hybrid_results['accuracy']],\n",
    "    'F1_Macro': [v['F1-Score (Macro)'] for v in results_summary['Traditional ML'].values()] + [hybrid_results['f1_macro']],\n",
    "    'F1_Weighted': [v['F1-Score (Weighted)'] for v in results_summary['Traditional ML'].values()] + [hybrid_results['f1_weighted']]\n",
    "})\n",
    "\n",
    "results_csv_path = os.path.join(config.results_dir, 'model_comparison_results.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"‚úÖ Results saved to: {results_csv_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = os.path.join(config.results_dir, 'experiment_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config.to_dict(), f, indent=2)\n",
    "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
    "\n",
    "print(\"\\nüéâ Experiment complete! All results and visualizations have been saved.\")\n",
    "print(f\"   üìÅ Results directory: {config.results_dir}\")\n",
    "print(f\"   üìä Figures directory: {config.figures_dir}\")\n",
    "print(f\"   ü§ñ Models directory: {config.models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
